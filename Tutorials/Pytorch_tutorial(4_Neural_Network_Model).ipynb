{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_tutorial(4_Neural Network Model)",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDvL98F7E42v"
      },
      "source": [
        "# 신경망 모델 구성하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUIVRRfEFKxg"
      },
      "source": [
        "신경망은 데이터에 대한 연산을 수행하는 계층(layer)/모듈(module)로 구성\n",
        "\n",
        "torch.nn 네임스페이스는 신경망을 구성하는데 필요한 모든 구성 요소를 제공\n",
        "\n",
        "- PyTorch의 모든 모듈은 nn.Module 의 하위 클래스(subclass)\n",
        "\n",
        "- 신경망은 다른 모듈(계층; layer)로 구성된 모듈\n",
        "\n",
        "- 이러한 중첩된 구조는 복잡한 아키텍처를 쉽게 구축하고 관리할 수 있다\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN-HaSJEEB14"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z78m1bboIMbk"
      },
      "source": [
        "# 장치 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_u_4CowIPjK"
      },
      "source": [
        "가능한 경우 GPU와 같은 하드웨어 가속기에서 모델을 학습하려고 합니다. torch.cuda 를 사용할 수 있는지 확인하고 그렇지 않으면 CPU를 계속 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj62uzmiEdn4",
        "outputId": "1ebfd4a5-bd63-4dc0-c621-b08b604ba56d"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg_GMoN4Iclh"
      },
      "source": [
        "# 클래스 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQtwyw19Ihi4"
      },
      "source": [
        "신경망 모델을 nn.Module 의 하위클래스로 정의하고, __init__ 에서 신경망 계층들을 초기화\n",
        "\n",
        "nn.Module 을 상속받은 모든 클래스는 forward 메소드에 입력 데이터에 대한 연산들을 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr5SK7j4Ef9V"
      },
      "source": [
        "class NeuralNetwork(nn.Module): # nn.Module의 sub class\n",
        "    def __init__(self): # 신경망 계층 초기화\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential( # relu 활성화 함수를 활용해 간단하게 쌓음\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEeVlkkfI1o0"
      },
      "source": [
        "NeuralNetwork 의 인스턴스(instance)를 생성하고 이를 device 로 이동한 뒤, 구조(structure)를 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PxTAeXSEi1C",
        "outputId": "9dc941bd-85f8-4031-e2b0-d2981dc36aa0"
      },
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWbt-s89Kxjo"
      },
      "source": [
        "모델을 사용하기 위해 입력 데이터를 전달, 이는 일부 백그라운드 연산들(https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866) 과 함께 모델의 forward 를 실행 \n",
        "\n",
        "model.forward() 를 직접 호출 XXX\n",
        "\n",
        "모델에 입력을 호출하면 각 분류(class)에 대한 원시(raw) 예측값이 있는 10-차원 텐서가 반환\n",
        "\n",
        " 원시 예측값을 nn.Softmax 모듈의 인스턴스에 통과시켜 예측 확률을 얻는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rXkYyE8Ek5M",
        "outputId": "999aba1a-a71d-4b0a-af4c-739bdf00d69b"
      },
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "print(logits)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0280, 0.0941, 0.0956, 0.0000, 0.0426, 0.0370, 0.0000, 0.0000, 0.0091,\n",
            "         0.0267]], grad_fn=<ReluBackward0>)\n",
            "Predicted class: tensor([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wz_b8TkLMuB"
      },
      "source": [
        "# 모델 계층(layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq4HVSWHLXzx"
      },
      "source": [
        "FashionMNIST 모델의 계층들을 살펴보기 위해, 28x28 크기의 이미지 3개로 구성된 미니배치를 가져오자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMDTX6aqEph2",
        "outputId": "cbc79591-f9f4-4ea7-f1ae-a542bd4d6a1e"
      },
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLuQencgLfQV"
      },
      "source": [
        "nn.Flatten 계층을 초기화하여 각 28x28의 2D 이미지를 784 픽셀 값을 갖는 연속된 배열로 변환 (dim=0의 미니배치 차원은 유지)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e36i0LtMEray",
        "outputId": "a07ed294-d714-430e-9dda-6fb0360e9081"
      },
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46cR2d-7LmGh"
      },
      "source": [
        "선형 계층 은 저장된 가중치(weight)와 편향(bias)을 사용하여 입력에 선형 변환(linear transformation)을 적용하는 모듈"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bvM1JStEtAD",
        "outputId": "06376c17-625a-402b-b229-5bb995ee850d"
      },
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcAmIKrzL1v4"
      },
      "source": [
        "비선형 활성화는 선형 변환 후에 적용되어 비선형성(nonlinearity) 을 도입하고, 신경망이 다양한 현상을 학습할 수 있도록 돕는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6PjtctkEuu_",
        "outputId": "37afb76d-4e64-4637-89be-c344df0c58a4"
      },
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before ReLU: tensor([[ 0.0754, -0.0920, -0.4195, -0.0816,  0.1585,  0.1059,  0.0859, -0.0237,\n",
            "         -0.4213, -0.3847,  0.2132, -0.6012, -0.0778,  0.2698,  0.1957,  0.3360,\n",
            "          0.4244,  0.2096, -0.1568,  0.3513],\n",
            "        [ 0.4669, -0.2308, -0.2152, -0.0756,  0.2111,  0.1579, -0.2932, -0.0664,\n",
            "         -0.5924, -0.4994, -0.0131, -0.1768,  0.0149,  0.0926,  0.2496,  0.2889,\n",
            "          0.6129,  0.1932, -0.1410,  0.2658],\n",
            "        [-0.0569, -0.1538, -0.2790, -0.1473,  0.2239, -0.1712, -0.1137, -0.2212,\n",
            "         -0.6220, -0.0491,  0.3332, -0.3355, -0.1989,  0.0284,  0.1783,  0.5151,\n",
            "          0.3050, -0.1781,  0.2577,  0.4806]], grad_fn=<AddmmBackward>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.0754, 0.0000, 0.0000, 0.0000, 0.1585, 0.1059, 0.0859, 0.0000, 0.0000,\n",
            "         0.0000, 0.2132, 0.0000, 0.0000, 0.2698, 0.1957, 0.3360, 0.4244, 0.2096,\n",
            "         0.0000, 0.3513],\n",
            "        [0.4669, 0.0000, 0.0000, 0.0000, 0.2111, 0.1579, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0149, 0.0926, 0.2496, 0.2889, 0.6129, 0.1932,\n",
            "         0.0000, 0.2658],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2239, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.3332, 0.0000, 0.0000, 0.0284, 0.1783, 0.5151, 0.3050, 0.0000,\n",
            "         0.2577, 0.4806]], grad_fn=<ReluBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJGxQb0mMNlV"
      },
      "source": [
        "nn.Sequential 은 순서를 갖는 모듈의 컨테이너\n",
        "\n",
        "- 데이터는 정의된 것과 같은 순서로 모든 모듈들을 통해 전달\n",
        "- 순차 컨테이너(sequential container)를 사용하여 아래의 seq_modules 와 같은 신경망을 만들 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DuGB2bfEwto"
      },
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9gfHtc0May8",
        "outputId": "553ffe2e-221e-44f9-d68e-2915233aa133",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(logits)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2496, -0.2913,  0.3128, -0.2472, -0.0322, -0.1965,  0.1731,  0.1055,\n",
            "         -0.0341, -0.3573],\n",
            "        [ 0.2149, -0.2263,  0.2322, -0.2594, -0.0179, -0.2542,  0.0123, -0.0184,\n",
            "          0.0583, -0.1947],\n",
            "        [ 0.1998, -0.2281,  0.2364, -0.1653,  0.0105, -0.0632,  0.2019,  0.1404,\n",
            "         -0.0704, -0.2384]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg9Nj9M1Eyot"
      },
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pF7KxmyE1F5",
        "outputId": "9598da4f-c5b7-4672-a06f-afac174a7f59"
      },
      "source": [
        "print(\"Model structure: \", model, \"\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model structure:  NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ") \n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0098, -0.0305, -0.0253,  ...,  0.0262,  0.0083,  0.0201],\n",
            "        [ 0.0349, -0.0177, -0.0262,  ..., -0.0335, -0.0310, -0.0109]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0164, 0.0315], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0113,  0.0330, -0.0127,  ...,  0.0246, -0.0378, -0.0024],\n",
            "        [ 0.0270,  0.0365,  0.0128,  ...,  0.0204, -0.0042,  0.0092]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0235, 0.0236], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0308,  0.0183, -0.0194,  ...,  0.0245,  0.0043,  0.0202],\n",
            "        [-0.0345, -0.0335, -0.0410,  ..., -0.0114,  0.0217, -0.0156]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0012, -0.0197], grad_fn=<SliceBackward>) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}